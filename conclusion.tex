This paper presents an end to end system for action classification which operates 
on both static and motion features. Our approach relies on deep features,
for creating static vectors, and \textit{motion tubes} for motion features. 
\textit{Motion tubes} are a novel concept we introduce in this paper which can be
used to track individual actors or objects across frames, and model micro level actions. 
We present three novel methods: Based on cholesky transformation, variance ratio, and PCA for efficient combining of features 
from different domains, which is a vital requirement in action classification.
Cholesky method provide the power to control the contribution of each domain in exact numbers, and
variance ratio based method mathematically provides an optimum ratio for contribution. We show that these mathematical 
and experimental values agree with each other.
We run experiments to show that the accuracy depends on the ratio of this contribution and the optimum contribution of 
static and motion domains may vary depending on the dataset. 

Through our experiments we also show that our static and motion features are complementory,
and contribute to the final result. We also compare our three fusion algorithms, and 
show that choleky is superioir, although all three of them give impressive results. 


We also model the temporal progression of subevents, using an LSTM network. Experimental 
results indicate that this is indeed benificary, compared to using models which does not capture temporal dynamics. 

Comparison of our work with mutiple state-of-the-art algorithms, on the 
two popular datasets, UCF-11 and Hollywood2, conclude the superiority of our proposed system.

In the future, it is interesting to improve the motion tubes, so that, it can maintain an identity over each actor object.
While, even at the present state, it is mostly the case, there is no mathematical guarentee for that. 
Also exploring more powerful methods to describe micro actions inside motion tubes is also interesting, since it may increase the descriptiveness of the motion features and
contribute to the final accuracy more powerfully. 
