Activity recognition in videos in a deep-learning setting use both static and pre-computed motion components. The method of combining the two components, whilst keeping the burden on the deep network less, still remains uninvestigated. Moreover, it is not clear what the level of contribution of individual components is and how to control the contribution. In this work, we use a combination of CNN-generated static features and motion features in the form of motion tubes. We propose three schemas for combining static and motion components: based on a variance ratio, principal components, and Cholesky decomposition. Cholesky decomposition based method allows the control of contributions. The ration given by variance analysis of static and motion features match well with the experimental optimal rations used in the Cholesky decomposition based method. The resulting activity recognition system is better or on par with existing state-of-the-art when tested with two popular data sets. The finding enables us to characterize data sets with respect to their richness in motion information. 