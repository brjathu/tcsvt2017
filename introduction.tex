
\IEEEPARstart{A}{u}tomatic action recognition in videos recieves research attention  
in computer vision mostly due to its wide range of real-world applications, such as, 
sports, health care, surveillance, robot vision, and human-computer interaction.
Furthermore, the rapid growth of digital video data, demands automatic
classification and indexing of videos. 


\tikzset{
  font={\fontsize{4pt}{5}\selectfont}}



Despite the increased interest, state-of-the-art
systems are still far from human performance, in contrast to the success in image classification \cite{girshick2014rich},
 \cite{krizhevsky2012imagenet}. This is partially due to the complex intra-class variations in videos. Some
obvious causes behind this are the view point, background 
clutter, high dimensionality of data, lack of larger datasets, and low resolution. 

Despite these reasons more-or-less affecting automatic image classification, it has been quite successful, in recent years,
largely owing to the rise of deep learning techniques. It is worthwhile investigating,
what is holding back video classification. In this study, mainly, three key
areas are addressed:exploiting underlying dynamics of sub events for high-level action reognition, crafting 
self explanatory and independant static and motion features(in which, motion features should capture micro-level actions
of each actor or object independantly, such as arm or leg movements), and optimum fusing of static and motion features for a better accuracy. 
These three aspects are briefly explained next.

A complex activity is typically composed of several sub activities. 
The existing approaches, try to classify a video, treating it as a 
single, high level activity \cite{wang2011action}, \cite{wang2013action}, \cite{simonyan2014two}, 
\cite{7486474}.
As the action becomes complex, the behavior and the 
temporal evolution of its underlying sub events becomes more complicated. For example,
cooking may involve sub actions: cutting, turning on the cooker, and stirring. It may not always preserve this 
same low level order for same action class. Rather, they may contain 
a higher order temporal relationship among them. For example, turning on cooker and cutting
may appear in reverse order in another video in the same class. 
Therefore, this temporal pattern of sub events, is not easy to capture through a simple
time series analysis. These patterns can only be identified by observing a large number
of examples, using a system, which has an infinite dynamic response. Therefore, it is critically important to
model this higher-order temporal
progression, and capture temporal dynamics of these sub events for better recognition of
complex activities. 

In contrast to image classification, the information contained in videos
are not 
 in a single domain. Both the motion patterns of the actors and objects, as well as the static
 information, such as, background, interactive or still objects, are important for determining an action.
For example, the body movements of a group of people fighting, may closely relate to 
body movements of a sports event. In such a case, it is tough to distinguish between the two 
activities solely by looking at the motion patterns. Inspecting the background setting 
and objects is crucial in such a scenario. Therefore, it is necessary to engineer powerful features
from both motion and static domains. In addition, these features must be complementary, and one feature domain should 
not influence the other domain. In other words, they should be self explanatory, and mutually exclusive, as much as possible. 
Also, these motion features should be able to capture
activities of each actor independently. If so, the distributional properties of these actions, over short
and long time ranges, can be used to identify high level actions. In the proposed method, we craft static and motion features to have this property. 

Furthermore, how to optimally combine these motion and static descriptors remains a challenge for three key reasons:both static and motion information
provide information regarding an action in a video, the contribution ratio from each domain affects the final recognition accuracy(proved by our experiments),
and optimum contribution ratio of static and motion information may depend on the dataset(proved by our experiments).
The combined descriptor should contain the essence of both domains, and should not have a 
negative interference on each other. Recently, there has been attempts to answer this question \cite{7486474},\cite{simonyan2014two}. However,
thes existing methodologies lack the insight in to how much this ratio affects the final accuracy, and has no control over this ratio. 
One major requirement of the fusion method, is to have a high-level intuitive interpretation on how much contribution each domain provides to the 
final descriptor, and have the ability to control level of contribution. This ability makes it possible
to deeply investigate the the optimum contribution of each domain for a better accuracy. In this study, we investigate this factor 
extensively.

In this work, we focus on the problematic areas:crafting mutually exclusive static and motion features, optimal fusion of the crafted features, and 
modeling temporal dynamics of sub activites for high-level action classification, and provide 
novel and efficient solutions. In order to examine the temporal evolution 
of sub activities later, we create video segments with constant overlapping durations. Afterwards, we combine static and motion
descriptors to represent each segment. We propose \textit{motion tubes},
a dense trajectory \cite{wang2011action} based tracking mechanism, to identify and track candidate moving areas
separately. This enables independent modeling of the activities in each moving area.
In order to capture motion patterns proficiently, we use histogram oriented optic flows (HOOF) \cite{chaudhry2009histograms}
inside motion tubes. Then we apply a bag-of-words method on the generated 
features to capture the distributional properties of micro actions, such as body movements, and create high level, discriminative motion features. 

Inspired by the power of convolutional neural networks, in the task of object recognition, we create a 7 layer deep
CNN, and pre-train it on the popular Imagenet dataset \cite{deng2012imagenet}.
Afterwards, we use this trained CNN to create deep features, for creating static descriptors. 

Then, using a computationally efficient, yet powerful mathematical model, we fuse these 
two feature vectors. We propose three such novel methods in this paper: based on
Cholesky decomposition, variance ratio of motion and static vectors, and principal components analysis(PCA). Choleskey decomposition based model provides the ability to 
control the contribution of static and motion domains,
to the final fused discriptor precisely. Using this intuition, we investigate the 
optimum contribution of each domain, experimentally.  Variance ratio based method also provides us this ability, and additionally, lets us
obtain the optimum contribution mathematically. We show that the optimum contribution ratio we obtained experimentally from Choleskey based method, 
matches with the ratio obtained mathematically from the variance based method. Furthermore, we show that this optimum contribution may
vary depending on the dataset and affects the final accuracy significantly. Additionally, our PCA based method also provides 
impressive resutls. 


In order to model the temporal progression of sub events, we feed the fused vectors to 
a LSTM network. The LSTM network discovers the underlying temporal patterns of the sub events, and classifies high level actions. 
We feed the same vectors to a classifier which does not capture temporal dynamics
to show that modeling temporal progression of sub events, indeed contribute for a better 
accuracy. 
=======

We carry out our experiments on the two popular action datasets, UCF-11 \cite{liu2009recognizing}
and Hollywood2 \cite{marszalek2009actions}. Our results surpasses existing best state-of-the-art
results on these datasets to the best of our knowledge.

The key contributions of this paper are as follows:

 \begin{itemize}
  \item We propose an end to end system, which simultaneously extracts static and motion information, fuses, and models the 
temporal evolution of sub activities, and does action classification. 
  \item We propose a novel, moving actor and object tracking mechanism, called \textit{motion tubes},
which enables the system to track each actor or object independently, and model the motion patterns individually. 
This allows the system to model actions, occurring in a video, in micro level, and use these learned dynamics
at a high level afterwards.
 \item We propose three novel, simple, and an efficient mathematical models for fusing two vectors, 
in two different 
domains, based on Choleskey transformation, variance ratio of motion and static vectors, and PCA. These methods provides 
the ability to govern the contribution of each domain
for the fused vector precisely and find the optimum contribution of the to domains, mathematically or empirically. Using this advantage, we prove that
both static and motion information are complementary and vital for activity recognition through experiments. Also, we show that optimum contribution 
may vary depending on the dataset, and affects the final accuracy significantly. 
 \item We prove the final recognition accuracy depends on the ratio of contribution of static and motion domains. Also we show that
 this optimum ratio depends on the dataset. Hence it is benificial to exploit this optimum ratio for a better accuracy. 
  \item We model the underlying 
temporal evolution of sub events for complex action recognition using a LSTM network. We experimentally 
prove that, capturing these dynamics, indeed benefits the final accuracy. 

\item With the proposed technique, and novel techniques,
we outperform the existing best results for the popular datasets UCF-11 \cite{liu2009recognizing}
and Hollywood2 \cite{marszalek2009actions}.
 \end{itemize}




